{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import torch\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.65278625 -1.5\n",
      "  -0.          0.          6.83172083  6.         -0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.4669857  -1.5\n",
      "   0.          0.         -6.83172083  6.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "\n",
    "# Testing\n",
    "next_states = np.reshape(env_info.vector_observations, (1, num_agents*state_size))\n",
    "print(next_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Overview of Solution\n",
    "\n",
    "### 3.1 Algorithm\n",
    "\n",
    "The actor critic algorithm selected to complete this task was Proximal Policy Optimization (PPO) with a clipped objective. This is an on-policy algorithm. In addition Generalized Advantage Estimation (GAE) has been imlemented. The implementation of the alogrithm has been based a PPO implementation by Phil Tabor (https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/PolicyGradient/PPO/torch) but has been adapted for both a continuous environment as well as the use of multiple agents.\n",
    "\n",
    "\n",
    "### 3.2 Network Design\n",
    "\n",
    "Separate networks have been produced for both the actor and critic, with no shared elements. This architecture showed similar or better performance than some other alternatives with a shared input layer(s). An overview of the architecture is given below. The actor policy generates actions stochastically from a normal distribution.\n",
    "\n",
    "Actor\n",
    "\n",
    "    s -> Linear(s, 128) -> ReLU -> Linear(128, 64) -> ReLU -> Linear(64, a) -> Tanh -> Normal Distribution,\n",
    "        where s and a is the state and action space sizes respectively.\n",
    "\n",
    "Critic\n",
    "\n",
    "    s -> Linear(s, 128) -> ReLU -> Linear(128, 64) -> ReLU -> Linear(64, 1) -> V(s)\n",
    "\n",
    "The logic for the actor and critic networks is implemented as class `ActorNetwork` and `CriticNetwork` within `ppo_original.py`.\n",
    "\n",
    "### 3.3 Hyperparameters\n",
    "\n",
    "Agent hyperparameters are passed to the constructor class `Agent` in `ppo_original.py`. The hyperparameter values have been selected by a limited optimization process of varying the values and determining the best values for stability and performance.\n",
    "\n",
    "| parameter                | value      | description                                                                   |\n",
    "|--------------------------|------------|-------------------------------------------------------------------------------|\n",
    "| optimizer_learning_rate  | 8e-4       | Learning rate for Adam optimizer                                              |\n",
    "| gamma                    | 0.99       | Discount rate for future rewards                                              |\n",
    "| gae_lambda               | 0.95       | Smoothing parameter for GAE                                                   |\n",
    "| n_epochs                 | 10         | Number of optimization steps to perform after trajectory rollback             |\n",
    "| batch_size               | 256        | Number of N-agent experiences to collect for a single optimization step       |\n",
    "| policy_clip              | 0.2        | Clipping parameter for the policy loss function                               |\n",
    "| value_loss_weight        | 0.5        | Weight applied to value loss on total loss function                           |\n",
    "\n",
    "The training hyperparameters defined below are passed to the training function, `train_agent`, defined within this workbook. \n",
    "\n",
    "| parameter                     | value     | description                                           |\n",
    "|-------------------------------|-----------|-------------------------------------------------------|\n",
    "| n_episodes                    | 5000      | Maximum number of training episodes                   |\n",
    "| N                             | 1024      | Trajectory length, Number of timesteps before         |\n",
    "|                                             performing a policy update                            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppo_original import Agent\n",
    "\n",
    "#Set agent hyperparameters\n",
    "alpha = 8e-4 #8e-4\n",
    "gamma = 0.99 #B-0.99\n",
    "gae_lambda = 0.95 #B-0.95 #Smoothing parameter\n",
    "n_epochs=10 #B-10\n",
    "batch_size=256 #B-256\n",
    "policy_clip = 0.2 #B-0.2\n",
    "value_loss_weight = 0.5 \n",
    "gradient_clip = 13 #B-13\n",
    "\n",
    "# Initialize agent\n",
    "agent = Agent(action_size, state_size, gamma, alpha, gae_lambda, policy_clip, batch_size, n_epochs, value_loss_weight, num_agents,\n",
    "             gradient_clip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent_(env, agent, n_episodes=300, max_t=250, \n",
    "    display_every=100, solved_score=0.5, N=250):\n",
    "    \n",
    "    st_time = time.time()\n",
    "    \n",
    "    scores = []\n",
    "    mean_scores = []\n",
    "    \n",
    "    scores_window = deque(maxlen=100)\n",
    "    max_mean = 0\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    n_steps = 0\n",
    "    \n",
    "    for i_episode in range(1, n_episodes + 1):    \n",
    "        \n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        \n",
    "        \n",
    "        score = np.zeros(num_agents)\n",
    "        episode_length_list = []\n",
    "        \n",
    "        states = env_info.vector_observations\n",
    "\n",
    "        first_solved = True\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            actions, probs, vals = agent.choose_action(states)\n",
    "            \n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            \n",
    "            next_states = env_info.vector_observations\n",
    "            \n",
    "            rewards = env_info.rewards      \n",
    "            \n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            n_steps += 1\n",
    "            \n",
    "            score += rewards\n",
    "            \n",
    "            agent.remember(states, actions, probs, vals, rewards, dones)\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            if n_steps % N ==0:\n",
    "            \n",
    "                agent.learn(n_steps)\n",
    "\n",
    "            if np.any(dones):\n",
    "                break\n",
    "                \n",
    "        max_score = np.max(score) # for each agent\n",
    "        scores_window.append(max_score)       # save most recent score\n",
    "        mean_scores.append(np.mean(scores_window))\n",
    "        scores.append(max_score)              # save most recent score for plotting\n",
    "        \n",
    "        print('\\rEpisode {}\\tMax score: {:.2f}'.format(\n",
    "            i_episode, max_score), end=\"\")\n",
    "        \n",
    "        if i_episode % display_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage score: {:.2f}'.format(\n",
    "                i_episode, np.mean(scores_window)))\n",
    "            \n",
    "        if np.mean(scores_window) >= solved_score and np.mean(scores_window) > max_mean:\n",
    "        # if score is greater than solved criteria, and greater than previous best score report and save\n",
    "        # aim is to allow training to continue to reach optimal policy possible\n",
    "        \n",
    "            if first_solved == True:\n",
    "            \n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage score: {:.2f} - saving model'.format(\n",
    "                    np.maximum(i_episode, 0), np.mean(scores_window)))\n",
    "                \n",
    "                first_solve = False\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                print('\\nHigher average score achieved at {:d} episodes!\\tAverage score: {:.2f} - saving model'.format(\n",
    "                    np.maximum(i_episode, 0), np.mean(scores_window)))\n",
    "                \n",
    "            \n",
    "            torch.save(agent.critic.state_dict(), 'critic_ppo.pth')\n",
    "            torch.save(agent.actor.state_dict(), 'actor_ppo.pth')\n",
    "            #agent.save_models\n",
    "            \n",
    "            max_mean = np.mean(scores_window)\n",
    "\n",
    "    \n",
    "        \n",
    "    print('\\nRun Time: {:.2f}s'.format(time.time() - st_time))\n",
    "    \n",
    "    return scores, mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage score: 0.02\n",
      "Episode 200\tAverage score: 0.02\n",
      "Episode 300\tAverage score: 0.02\n",
      "Episode 400\tAverage score: 0.02\n",
      "Episode 500\tAverage score: 0.03\n",
      "Episode 600\tAverage score: 0.04\n",
      "Episode 700\tAverage score: 0.05\n",
      "Episode 800\tAverage score: 0.07\n",
      "Episode 900\tAverage score: 0.08\n",
      "Episode 1000\tAverage score: 0.10\n",
      "Episode 1100\tAverage score: 0.11\n",
      "Episode 1200\tAverage score: 0.14\n",
      "Episode 1300\tAverage score: 0.18\n",
      "Episode 1400\tAverage score: 0.24\n",
      "Episode 1500\tAverage score: 0.39\n",
      "Episode 1524\tMax score: 2.60\n",
      "Environment solved in 1524 episodes!\tAverage score: 0.50 - saving model\n",
      "Episode 1525\tMax score: 2.60\n",
      "Environment solved in 1525 episodes!\tAverage score: 0.52 - saving model\n",
      "Episode 1526\tMax score: 0.90\n",
      "Environment solved in 1526 episodes!\tAverage score: 0.53 - saving model\n",
      "Episode 1528\tMax score: 2.60\n",
      "Environment solved in 1528 episodes!\tAverage score: 0.55 - saving model\n",
      "Episode 1530\tMax score: 1.70\n",
      "Environment solved in 1530 episodes!\tAverage score: 0.56 - saving model\n",
      "Episode 1531\tMax score: 1.10\n",
      "Environment solved in 1531 episodes!\tAverage score: 0.57 - saving model\n",
      "Episode 1534\tMax score: 1.20\n",
      "Environment solved in 1534 episodes!\tAverage score: 0.57 - saving model\n",
      "Episode 1535\tMax score: 0.10\n",
      "Environment solved in 1535 episodes!\tAverage score: 0.57 - saving model\n",
      "Episode 1536\tMax score: 2.50\n",
      "Environment solved in 1536 episodes!\tAverage score: 0.59 - saving model\n",
      "Episode 1538\tMax score: 2.10\n",
      "Environment solved in 1538 episodes!\tAverage score: 0.61 - saving model\n",
      "Episode 1539\tMax score: 1.90\n",
      "Environment solved in 1539 episodes!\tAverage score: 0.63 - saving model\n",
      "Episode 1541\tMax score: 0.60\n",
      "Environment solved in 1541 episodes!\tAverage score: 0.63 - saving model\n",
      "Episode 1543\tMax score: 0.50\n",
      "Environment solved in 1543 episodes!\tAverage score: 0.63 - saving model\n",
      "Episode 1544\tMax score: 0.80\n",
      "Environment solved in 1544 episodes!\tAverage score: 0.64 - saving model\n",
      "Episode 1553\tMax score: 2.60\n",
      "Environment solved in 1553 episodes!\tAverage score: 0.66 - saving model\n",
      "Episode 1554\tMax score: 1.50\n",
      "Environment solved in 1554 episodes!\tAverage score: 0.67 - saving model\n",
      "Episode 1556\tMax score: 2.60\n",
      "Environment solved in 1556 episodes!\tAverage score: 0.68 - saving model\n",
      "Episode 1557\tMax score: 1.80\n",
      "Environment solved in 1557 episodes!\tAverage score: 0.70 - saving model\n",
      "Episode 1559\tMax score: 2.60\n",
      "Environment solved in 1559 episodes!\tAverage score: 0.72 - saving model\n",
      "Episode 1560\tMax score: 2.70\n",
      "Environment solved in 1560 episodes!\tAverage score: 0.75 - saving model\n",
      "Episode 1562\tMax score: 0.50\n",
      "Environment solved in 1562 episodes!\tAverage score: 0.75 - saving model\n",
      "Episode 1563\tMax score: 1.50\n",
      "Environment solved in 1563 episodes!\tAverage score: 0.76 - saving model\n",
      "Episode 1564\tMax score: 1.40\n",
      "Environment solved in 1564 episodes!\tAverage score: 0.77 - saving model\n",
      "Episode 1572\tMax score: 2.70\n",
      "Environment solved in 1572 episodes!\tAverage score: 0.77 - saving model\n",
      "Episode 1574\tMax score: 2.60\n",
      "Environment solved in 1574 episodes!\tAverage score: 0.79 - saving model\n",
      "Episode 1575\tMax score: 0.30\n",
      "Environment solved in 1575 episodes!\tAverage score: 0.80 - saving model\n",
      "Episode 1576\tMax score: 0.60\n",
      "Environment solved in 1576 episodes!\tAverage score: 0.80 - saving model\n",
      "Episode 1581\tMax score: 2.60\n",
      "Environment solved in 1581 episodes!\tAverage score: 0.81 - saving model\n",
      "Episode 1582\tMax score: 2.70\n",
      "Environment solved in 1582 episodes!\tAverage score: 0.83 - saving model\n",
      "Episode 1586\tMax score: 0.30\n",
      "Environment solved in 1586 episodes!\tAverage score: 0.83 - saving model\n",
      "Episode 1590\tMax score: 0.20\n",
      "Environment solved in 1590 episodes!\tAverage score: 0.83 - saving model\n",
      "Episode 1600\tAverage score: 0.80\n",
      "Episode 1700\tAverage score: 0.13\n",
      "Episode 1800\tAverage score: 0.40\n",
      "Episode 1858\tMax score: 2.60\n",
      "Environment solved in 1858 episodes!\tAverage score: 0.85 - saving model\n",
      "Episode 1860\tMax score: 2.60\n",
      "Environment solved in 1860 episodes!\tAverage score: 0.86 - saving model\n",
      "Episode 1861\tMax score: 2.60\n",
      "Environment solved in 1861 episodes!\tAverage score: 0.88 - saving model\n",
      "Episode 1862\tMax score: 0.59\n",
      "Environment solved in 1862 episodes!\tAverage score: 0.89 - saving model\n",
      "Episode 1863\tMax score: 1.30\n",
      "Environment solved in 1863 episodes!\tAverage score: 0.90 - saving model\n",
      "Episode 1864\tMax score: 1.90\n",
      "Environment solved in 1864 episodes!\tAverage score: 0.92 - saving model\n",
      "Episode 1865\tMax score: 2.40\n",
      "Environment solved in 1865 episodes!\tAverage score: 0.93 - saving model\n",
      "Episode 1866\tMax score: 2.60\n",
      "Environment solved in 1866 episodes!\tAverage score: 0.96 - saving model\n",
      "Episode 1867\tMax score: 2.70\n",
      "Environment solved in 1867 episodes!\tAverage score: 0.98 - saving model\n",
      "Episode 1868\tMax score: 0.80\n",
      "Environment solved in 1868 episodes!\tAverage score: 0.99 - saving model\n",
      "Episode 1869\tMax score: 2.60\n",
      "Environment solved in 1869 episodes!\tAverage score: 1.01 - saving model\n",
      "Episode 1900\tAverage score: 0.78\n",
      "Episode 2000\tAverage score: 0.00\n",
      "Episode 2100\tAverage score: 0.00\n",
      "Episode 2200\tAverage score: 0.00\n",
      "Episode 2300\tAverage score: 0.00\n",
      "Episode 2400\tAverage score: 0.00\n",
      "Episode 2500\tAverage score: 0.00\n",
      "Episode 2600\tAverage score: 0.00\n",
      "Episode 2700\tAverage score: 0.00\n",
      "Episode 2800\tAverage score: 0.00\n",
      "Episode 2900\tAverage score: 0.00\n",
      "Episode 3000\tAverage score: 0.00\n",
      "Episode 3100\tAverage score: 0.00\n",
      "Episode 3200\tAverage score: 0.00\n",
      "Episode 3300\tAverage score: 0.00\n",
      "Episode 3400\tAverage score: 0.00\n",
      "Episode 3500\tAverage score: 0.00\n",
      "Episode 3600\tAverage score: 0.00\n",
      "Episode 3700\tAverage score: 0.00\n",
      "Episode 3800\tAverage score: 0.00\n",
      "Episode 3900\tAverage score: 0.00\n",
      "Episode 4000\tAverage score: 0.00\n",
      "Episode 4100\tAverage score: 0.00\n",
      "Episode 4200\tAverage score: 0.00\n",
      "Episode 4300\tAverage score: 0.00\n",
      "Episode 4400\tAverage score: 0.00\n",
      "Episode 4500\tAverage score: 0.00\n",
      "Episode 4600\tAverage score: 0.00\n",
      "Episode 4700\tAverage score: 0.00\n",
      "Episode 4800\tAverage score: 0.00\n",
      "Episode 4900\tAverage score: 0.00\n",
      "Episode 5000\tAverage score: 0.00\n",
      "\n",
      "Run Time: 487.77s\n"
     ]
    }
   ],
   "source": [
    "scores, mean_scores = train_agent_(env, agent, n_episodes=5000, solved_score = 0.5, N=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores, label='Max score', color='blue')\n",
    "plt.plot(np.arange(len(scores)), mean_scores, label='Avg score', color='orange')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode No.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 2.3000000342726707\n",
      "904\n",
      "Score (max over agents) from episode 2: 0.800000011920929\n",
      "334\n",
      "Score (max over agents) from episode 3: 2.600000038743019\n",
      "1001\n",
      "Score (max over agents) from episode 4: 0.800000011920929\n",
      "286\n",
      "Score (max over agents) from episode 5: 2.0000000298023224\n",
      "786\n",
      "Score (max over agents) from episode 6: 2.500000037252903\n",
      "962\n",
      "Score (max over agents) from episode 7: 0.4000000059604645\n",
      "142\n",
      "Score (max over agents) from episode 8: 2.600000038743019\n",
      "1001\n",
      "Score (max over agents) from episode 9: 2.7000000402331352\n",
      "1001\n",
      "Score (max over agents) from episode 10: 0.0\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# load the saved checkpoint files\n",
    "agent.critic.load_state_dict(torch.load('critic_ppo.pth'))\n",
    "agent.actor.load_state_dict(torch.load('actor_ppo.pth'))\n",
    "agent.critic.eval()\n",
    "agent.actor.eval()\n",
    "\n",
    "\n",
    "for i in range(1, 10 + 1):                                      # play game for 5 episodes\n",
    "    t = 0\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions, _, _ = agent.choose_action(states)\n",
    "     \n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        t+=1\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))\n",
    "    print(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results & Conclusions & Future Work\n",
    "\n",
    "The selected algorithm was successful in solving the environment within the requirements with a fast timeframe (<10 mins on high end consumer PC with a RTX2080 Ti). The PPO algorithm demonstrated a reasonable level of robustness but was found to be very sensitive to changes in hyperparameters, with the documented hyperparameters giving the best performance. It was noted that in some cases the environment wasn't solved with the stated architecture/hyperparameters but subsequent runs would solve.\n",
    "\n",
    "Some ideas for future work include:\n",
    "- Further improving the algorithm to improve robustness to avoid failed runs in some cases (i.e. where model does not learn).\n",
    "- Performance comparison to other algorithms, for example the DDPG benchmark.\n",
    "- Review of further improvements to PPO algorithm to improve performance - i.e. input and advantage normalization, modifications to the clipping function suggested in some continuous control papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
